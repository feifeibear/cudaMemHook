## TurboHooker Design Doc

TurboHooker是一个GPU多进程内存管工具。

由于cuda内存分配释放开销很大，GPU上的显存管理是一个尚未完全解决的开放问题。具体到深度学习应用场景，在单个GPU上服务多个模型并没有最佳方案。

一些场景下显存的分配会造成的资源瓶颈。

单进程处理变长输入需求。在变长输入的DNN模型推理过程中，中间张量尺寸不断变化。
如果每次都通过立即分配释放内存，则会造成瓶颈。更明智的做法是，采用缓存策略避免频繁内存分配释放开销。

多进程服务需求。但GPU上运行多个进程。这些进程既可以是同一个模型的不同实例，也可以是不同模型的不同实例。
对于前者，我们往往可以诉诸于Batching策略优化吞吐率。
对于后者，我们需要唤醒多个进程，来服务不同类型请求。
比如某FAQ服务需要同时加载14个BERT模型服务不同分支逻辑。
这时为每个模型维护一个中间张量缓存会造成内存使用爆炸。

这些场景的出现，让我们思考是否可以把多进程内存统一管理，来彻底消除内存的分配释放开销，并且全局优化最优的内存用量。

TurboHooker采用client-server模式，server是一个守护进程(deamon)，在启动时候预先将GPU的全部内存分配出来。
TurboHooker拦截client的内存分配/释放请求，将它发送给server，通过运行一个调度器来寻找空闲的连续地址空间给client使用。
这样每次内存分配，只需套运行调度逻辑，而不是真正执行cudaMalloc/cuFree操作。

### Server调度算法
可以参考best fit类型算法，也可以结合问题特点实现model-aware的算法。

### Server-Client通信
GPU进程间通信需要使用cuda ipc，但是cuda ipc相比cpu ipc要低效很多。
必须避免每次内存请求调用cuda ipc。我们可以在client启动时，向它发送server的显存handle。
对于client的内存分配请求，server只需要client发送内存的地址偏移，这里可以使用mmap实现进程间高效通信。

### 隐私
不同进程之间地址应该隔离，避免进程通过越界地址访问其他进程的地址空间。有待思考。

### 应用场景
最典型场景是单机多进程推理服务，训练任务和推理任务混合调度等。
